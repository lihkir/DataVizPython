{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errores comunes al crear visualizaciones interactivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Objetivos\n",
    "- Identificar los errores que se cometen al crear visualizaciones\n",
    "- Aplicar técnicas para **corregir los errores y crear visualizaciones eficaces**\n",
    "- Seleccionar y **diseñar las visualizaciones adecuadas para determinados tipos de datos**\n",
    "- Describir las diferentes bibliotecas y herramientas disponibles para producir visualizaciones\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En éste capítulo se enumera y explica los posibles fallos y errores que se cometen durante varias etapas del proceso de visualización de datos, como la **visualización de elementos no correlacionados de un conjunto de datos** para mostrar una relación o **crear una característica interactiva inapropiada**.\n",
    "  \n",
    "- El proceso de visualización de datos puede parecer sencillo: tome algunos datos, trace algunos gráficos, añadir algunas características interactivas, y el trabajo está hecho. O, tal vez, no lo sea: podría haber varios lugares durante el trayecto en los que se pueden cometer errores. Estos errores terminan por dar lugar a una **visualización defectuosa que no puede de manera fácil y eficaz transmitir lo que dicen los datos**, lo que hace que sea completamente inútil para el público que lo está viendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formato e interpretación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La primera fase de la visualización de datos es **entender los datos frente a ti: entender lo que es, lo que significa y lo que transmite**. Solo cuando entiendas los datos serás capaz de diseñar una visualización que ayude a otros a entenderlos.\n",
    "\n",
    "- Además, es importante **asegurarse de que los datos tienen sentido y contienen suficiente información, ya sea categórica, numérica o una mezcla de ambas**, para ser visualizados. Por lo tanto, si tratas con datos erróneos o sucios, es probable que la visualización sea defectuosa.\n",
    "\n",
    "- En esta siguiente sección, veremos algunas formas de evitar los errores comunes que se suelen cometidos en esta fase de los datos y cómo evitarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo evitar los errores más comunes al tratar con datos sucios?**\n",
    "\n",
    "- **Garbage In, Garbage Out**: Este es un dicho popular en el campo de la ciencia de datos, especialmente con respecto a la visualización de datos. Básicamente, significa que **si utilizas datos desordenados y ruidosos, vas a obtener una visualización defectuosa y poco informativa**. Los datos desordenados, ruidosos y sucios corresponden a una serie de problemas que se encuentran en los datos. Vamos a discutir los problemas uno por uno y las formas de tratar este tipo de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valores atípicos**\n",
    "\n",
    "- Los datos que contienen valores inexactos o instancias que son *significativamente diferentes del resto de los datos de un dataset* se denominan valores atípicos. Estos valores atípicos pueden ser auténticos, es decir, parecen incorrectos, pero en realidad no lo son, o son **errores que se cometen al recoger o almacenar los datos**.\n",
    "\n",
    "- Veamos un ejemplo de un error cometido al recoger o almacenar los datos. La siguiente tabla muestra la *edad, el peso y el sexo de los clientes que acuden a un determinado gimnasio*. La columna de sexo consta de tres valores discretos *0, 1 y 2 que corresponden todos a una clase: hombre, mujer y otro*, respectivamente. La columna de la edad se expresa en años y la columna de peso está en kilogramos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./figures/Figure701.png\n",
    ":align: center\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Todo parece correcto hasta que llegamos a la cuarta instancia (índice 3), donde el peso es de 790 kg. *Esto parece extraño porque nadie puede pesar realmente 790 kg, especialmente alguien cuya estatura es de 1,5 metros y 7 pulgadas*. Quien haya almacenado estos datos *debe haber querido decir 79 kg y haber añadido un 0 por error*.\n",
    "\n",
    "- Este es un caso de un valor atípico en el conjunto de datos. Esto puede parecer trivial en este momento, sin embargo, *esto puede resultar en visualizaciones y predicciones o patrones de modelos de aprendizaje automático defectuosos*, especialmente si hay múltiples repeticiones de esos datos. Ahora, veamos un ejemplo de un auténtico valor atípico en la siguiente tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./figures/Figure702.png\n",
    ":align: center\n",
    ":scale: 80\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El peso en el cuarto caso (índice 3) es de *167 kilogramos, lo que parece extrañamente alto. Sin embargo, sigue siendo un valor verosímil, ya que es posible que alguien tenga un peso de 167 kilogramos a los 37 años*. Por lo tanto, se trata de un *verdadero valor atípico*. Mientras que en los ejemplos anteriores es fácil detectar el valor atípico, ya que solo hay 5 casos, en realidad, nuestros conjuntos de datos son masivos, por lo que comprobar cada caso es una tarea tediosa y poco práctica.\n",
    "- En consecuencia, en la vida real, podemos utilizar visualizaciones estáticas básicas, como los *gráficos de caja*, para observar la existencia de valores atípicos. Los gráficos de caja son visualizaciones de datos sencillas, pero informativas que pueden decirnos mucho sobre la forma en que se distribuyen nuestros datos. Muestran el rango de nuestros datos basándose en cinco valores clave:\n",
    "   \n",
    "    - El valor mínimo de la columna\n",
    "    - El primer cuartil\n",
    "    - La mediana\n",
    "    - El tercer cuartil\n",
    "    - El valor máximo de la columna\n",
    "      \n",
    "- Esto es lo que hace que sean excelentes para **mostrar los valores atípicos, además de describir la simetría de los datos, el grado de agrupación de los mismos** (si todos los valores están repartidos en un amplio rango), **y si están o no sesgados**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de valores atípicos en un conjunto de datos con un gráfico de cajas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En este ejercicio, vamos a crear un gráfico de caja para comprobar si nuestro conjunto de datos contiene valores atípicos. Vamos a utilizar el conjunto de datos *gym.csv*, que contiene información sobre los clientes de un determinado gimnasio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guarda el archivo *gym.csv* en un DataFrame llamado **gym**, e imprime las cinco primeras filas del mismo para ver cómo son los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym = pd.read_csv('https://raw.githubusercontent.com/lihkir/Uninorte/main/AppliedStatisticMS/DataVisualizationRPython/Lectures/Python/PythonDataSets/gym.csv')\n",
    "gym.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como puede ver, nuestros datos tienen tres columnas: `age, weight, sex`. La columna sexo consta de tres valores discretos que corresponden a tres clases discretas: `0 es hombre`, `1 es mujer` y `2 es otro`.\n",
    "\n",
    "- Cree un gráfico de caja con el eje $x$ como columna de sexo y el eje $y$ como peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(gym, x = 'sex', y = 'weight', notched = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La escala del eje $y$ es extrañamente grande, ya que todos los **gráficos de caja están comprimidos en la octava parte inferior del gráfico**, por lo que no representa una visualización clara de los datos. Esto se debe al valor atípico en la cuarta instancia de nuestro `DataFrame` **790 kg**\n",
    "- Todos los valores parecen estar bien, excepto ese valor atípico en la parte superior del gráfico con **max=790**. Ahora, veremos las formas de tratar los valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómo tratar los valores atípicos**\n",
    "\n",
    "Hay tres formas principales de tratar los valores atípicos:\n",
    "\n",
    "- **Eliminación**: Si sólo hay unas pocas instancias (filas) que poseen valores atípicos, entonces esas instancias pueden ser eliminadas completamente del conjunto de datos, dejando así un conjunto de datos con cero valores atípicos. También hay ocasiones en las que una determinada característica (columna) contiene un gran número de valores atípicos. En tal caso, esa característica concreta puede eliminarse del conjunto de datos pero sólo si esa característica es insignificante. Sin embargo, la eliminación de datos no siempre es la mejor idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Imputación**: La imputación es una opción mejor que la eliminación, especialmente **si hay muchos valores atípicos en el conjunto de datos**. Esto puede hacerse de tres maneras:\n",
    "    - La forma más común es imputar los valores atípicos con la **media**, la **mediana** o la **moda** de la columna. Sin embargo, en el caso de muchos valores atípicos, estos valores pueden no ser lo suficientemente bueno, ya que cada valor atípico será un problema, debido a que cada uno de estos valores será sustituido por el mismo valor (la **media**, la **mediana** o la **moda**)\n",
    "    - El otro método para obtener mejores valores para los valores atípicos, especialmente en el caso de **series temporales**, es la **interpolación lineal**, es decir, el uso de polinomios lineales para crear nuevos puntos de datos dentro de un rango definido de puntos conocidos para reemplazar valores atípicos.\n",
    "    - Un modelo de **regresión lineal** también puede utilizarse para **predecir un valor que falta si es numérico**, y en el caso de que el **valor que falta sea categórico**, se puede utilizar un modelo de **regresión logística**.\n",
    "      \n",
    "    - Por ejemplo, supongamos que tiene un conjunto de datos del que necesita mostrar una relación entre la altura y el peso. La columna de la altura tiene varios valores perdidos, pero, como es una **característica significativa, no puede eliminarla, ni tampoco puede imputar la media de la columna**, ya que eso podría llevar a una relación falsa. El conjunto de datos puede dividirse en dos conjuntos de datos\n",
    "        - El conjunto de datos de entrenamiento, que contiene instancias sin valores perdidos\n",
    "        - El nuevo conjunto de datos, que contiene solo los casos en los que faltan valores en la columna de altura.\n",
    "        - A continuación, se puede utilizar un modelo de regresión lineal en el conjunto de datos de entrenamiento. El modelo aprenderá de estos datos y, cuando se le proporcione el nuevo conjunto de datos, podrá **predecir los valores de la columna de altura**. Ahora, los dos conjuntos de datos pueden fusionarse juntos y ser utilizados para crear visualizaciones, ya que no hay valores perdidos.\n",
    "\n",
    "- **Transformación**: Es el proceso de transformación de los valores atípicos mediante la construcción de la columna de datos en la que se encuentra el valor atípico, por ejemplo, convirtiendo los valores en porcentajes y utilizando esa columna como característica en lugar de la columna original. En la siguiente sección, veremos un ejercicio para entender cómo podemos tratar los valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo tratar los valores atípicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detección de valores atípicos**\n",
    "\n",
    "- Si nuestro conjunto de datos es pequeño, podemos detectar el valor atípico simplemente mirando el conjunto de datos. Pero si tenemos un conjunto de datos enorme, ¿cómo podemos identificar los valores atípicos? Tenemos que utilizar técnicas de visualización y matemáticas. A continuación se presentan algunas de las técnicas de detección de valores atípicos\n",
    "\n",
    "    - Boxplots\n",
    "    - Puntuación $Z$\n",
    "    - Rango intercuantil **(IQR)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detección de valores atípicos mediante las puntuaciones $Z$**\n",
    "\n",
    "- **Criterio**: Cualquier dato cuya puntuación $Z$ esté **fuera de la tercera desviación estándar es un valor atípico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./figures/z_score.png\n",
    ":align: center\n",
    ":scale: 80\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementación**:\n",
    "\n",
    "- Recorrer todos los datos y calcular la puntuación $Z$ mediante la fórmula $(X_{i}-\\mu)/\\sigma$. Definir un valor de umbral de 3 y marcar los puntos de datos cuyo valor absoluto de puntuación $Z$ sea mayor que el umbral como valores atípicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def detect_outliers_zscore(data):\n",
    "    thres = 3\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    for i in data:\n",
    "        z_score = (i - mean)/std\n",
    "        if (np.abs(z_score) > thres):\n",
    "            outliers.append(i)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_outliers = detect_outliers_zscore(gym.weight)\n",
    "print(\"Outliers from Z-scores method: \", sample_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detección de valores atípicos mediante el rango intercuantil (IQR)**\n",
    "\n",
    "- **Criterio**: Los datos que se sitúan 1.5 veces del **IQR** por encima de $Q_{3}$ y por debajo de $Q_{1}$ son valores atípicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementación**\n",
    "\n",
    "- Ordenar el conjunto de datos de forma ascendente\n",
    "- Calcular los cuartiles 1 y 3 $(Q_{1}, Q_{3})$\n",
    "- Calcular $IQR=Q_{3}-Q_{1}$\n",
    "- Calcular el límite inferior = $(Q_{1}-1.5*IQR)$, el límite superior = $(Q_{3}+1.5*IQR)$, recorrer los valores del conjunto de datos y comprobar los que están por debajo del límite inferior y por encima del límite superior y marcarlos como valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def detect_outliers_iqr(data):\n",
    "    data = sorted(data)\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    IQR = q3-q1\n",
    "    lwr_bound = q1-(1.5*IQR)\n",
    "    upr_bound = q3+(1.5*IQR)\n",
    "    for i in data: \n",
    "        if (i<lwr_bound or i>upr_bound):\n",
    "            outliers.append(i)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_outliers = detect_outliers_iqr(gym.weight)\n",
    "print(\"Outliers from IQR method: \", sample_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de los valores atípicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminación**\n",
    "\n",
    "- En este ejercicio, vamos a eliminar la instancia que contiene el valor atípico del conjunto de datos que utilizamos en el ejercicio previo, y visualizamos el conjunto de datos de nuevo generando un gráfico de caja basado en el nuevo conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guarda el archivo *gym.csv* en un DataFrame llamado **gym**, e imprime las cinco primeras filas del mismo para ver cómo son los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym = pd.read_csv('https://raw.githubusercontent.com/lihkir/Uninorte/main/AppliedStatisticMS/DataVisualizationRPython/Lectures/Python/PythonDataSets/gym.csv')\n",
    "gym.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modificar el DataFrame del gimnasio para que solo esté formado por los *casos en los que el peso sea inferior a 104* e imprimir las cinco primeras filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_del = gym[gym.weight < 104]\n",
    "gym_del.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a crear un boxplot para ver el aspecto de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.box(gym_del, x = 'sex', y = 'weight', notched = True)\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputación por medio de la *media, mediana o moda***\n",
    "\n",
    "- Como el *valor medio es sensible a valores atípicos, se aconseja sustituirlos por la mediana*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = np.median(gym.weight)\n",
    "print(\"median: \", median)\n",
    "\n",
    "for outlier in sample_outliers:\n",
    "    gym.weight = np.where(gym.weight == outlier, median, gym.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.box(gym, x = 'sex', y = 'weight', notched = True)\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de los valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos faltantes**\n",
    "\n",
    "- Los datos perdidos son, como su nombre indica, valores que están en blanco (**NaN, -, 0** cuando no deberían ser **0**, etc.). Al igual que los valores atípicos, los valores perdidos pueden ser problemáticos tanto en el caso de las visualizaciones como en el de los modelos predictivos.\n",
    "\n",
    "- Los valores faltantes en las visualizaciones pueden mostrar una tendencia que en realidad no existe o no representa una relación entre dos variables que, en realidad, es significativa. Aunque es posible crear visualizaciones con un conjunto de datos que contenga valores perdidos, no se recomienda hacerlo. Al hacerlo, se ignoran los casos en los que se encuentran esos valores perdidos, creando así una visualización basada en algunos de los datos pero no en todos. \n",
    "\n",
    "- Por lo tanto, el tratamiento de los valores perdidos es de suma importancia. Existen dos enfoques principales para tratar los valores perdidos: `la supresión y la imputación, ambos discutidos en términos de tratamiento de los valores atípicos. La misma lógica se aplica a los valores faltantes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En este ejercicio, vamos a trabajar con un conjunto de datos que tiene `siete valores perdidos en forma de 0s`. En primer lugar, `eliminaremos las instancias que contienen estos valores perdidos y se genera un gráfico de caja para ver el impacto que la eliminación de un gran número de instancias en nuestra visualización`. A continuación, imputaremos el valor de la mediana de la columna que contiene los valores perdidos y generaremos un gráfico de caja basado en este conjunto de datos imputados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guarda el archivo *weight.csv* en un `DataFrame` llamado **weight**, e imprime las cinco primeras filas del mismo para ver cómo son los datos y utiliza la función `.describe()` para mostrar información sobre ella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.read_csv('https://raw.githubusercontent.com/lihkir/Uninorte/main/AppliedStatisticMS/DataVisualizationRPython/Lectures/Python/PythonDataSets/weight.csv')\n",
    "w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como podemos ver, el valor de peso mínimo en nuestro conjunto de datos es **0**; sin embargo, nadie puede pesar **0 kgs**, lo que significa que tenemos valores faltantes en forma de **0s**. Intentemos eliminar estas instancias.\n",
    "\n",
    "- Cree un nuevo `DataFrame` que conste sólo de las instancias en las que el peso no sea igual a **0**. Muestre información sobre este nuevo `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_w = w[w.weight != 0]\n",
    "doc_w.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crea un `boxplot` con este nuevo `DataFrame`, con el eje $x$ como sexo y el eje $y$ como peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.box(doc_w, x = 'sex', y = 'weight', notched = True)\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora, el valor del peso mínimo es 21, lo que tiene más sentido. Sin embargo, nuestro conteo se ha reducido a `55 de 62, lo que significa que se han eliminado 7 instancias de nuestro conjunto de datos`. Esto puede parecer pequeño en este ejemplo, pero en realidad, esto puede tener repercusiones serias en la información obtenida.\n",
    "\n",
    "- Además, en el gráfico de caja anterior, el extremo inferior de la caja para el sexo 0 y el extremo superior de la caja para el sexo 2 es ligeramente anormal. Por lo tanto, sustituyamos los valores 0 de la columna de peso por el valor medio de la columna. Recuerde que tenemos que calcular la media de la columna **¡columna sin tener en cuenta esos valores 0!** Si los tenemos en cuenta, entonces nuestra media será incorrecta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calcular la media de la columna de pesos del `DataFrame` que consiste en sólo valores de peso distintos de cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_w = doc_w['weight'].median()\n",
    "median_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilice la función `.replace()` para sustituir los valores **0** presentes en la columna **weight** del `DataFrame` original por la mediana de la columna **weight** del modificado. Guarde esto en un nuevo `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_new = w.replace({'weight': {0: median_w}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Muestra la información del nuevo `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nuestro recuento es de 62, lo que significa que tenemos todas las instancias, y nuestro peso mínimo es 21, lo que significa que no tenemos ningún **0**.\n",
    "\n",
    "- Cree un gráfico de caja con este nuevo `DataFrame`, con el eje $x$ como **sex** y el eje $y$ como **weight**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = px.box(w_new, x = 'sex', y = 'weight', notched = True)\n",
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación múltiple iterativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora, tenemos una visualización que no tiene valores perdidos y representa todas las instancias que están presentes en el conjunto de datos. Veamos el tercer problema que puede generar una visualización defectuosa. **Otras técnicas de imputación de datos fueron estudiadas en R, y se pueden importar a Python**. En Python, `MICE` se puede ejecutar en un paquete llamado [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html), el cual está en fase experimental y está disponible en el paquete `sklearn.impute`.\n",
    "\n",
    "- La siguiente figura describe como funciona la imputación múltiple iterativa, la cual seguiremos analizando también usando el lenguaje `R`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./figures/multiple_imputation.png\n",
    ":name: multiple_imputation_fig\n",
    ":align: center\n",
    "\n",
    "Funcionamiento del método de imputación múltiple por ecuaciones encadenadas.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./figures/pmm_imputation.png\n",
    ":name: pmm_imputation_fig\n",
    ":align: center\n",
    "\n",
    "Funcionamiento del método de imputación por emparejamiento predictivo medio.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El conjunto de datos del presente ejemplo, describe las características médicas de caballos con cólico y si sobrevivieron o murieron (ver [horse-colic](https://raw.githubusercontent.com/lihkir/Data/main/horse-colic.names.txt)). Hay 300 filas y 26 variables de entrada con una variable de salida. Es una tarea de predicción de **clasificación binaria** que implica predecir *1 si el caballo vivió y 2 si el caballo murió*.\n",
    "\n",
    "- Hay muchos campos que podríamos seleccionar para predecir en este conjunto de datos. En este caso, **predeciremos si el problema fue quirúrgico o no (índice de columna 23)**, convirtiéndolo en un problema de clasificación binaria. El conjunto de datos tiene muchos valores faltantes para muchas de las columnas, donde **cada valor faltante está marcado con un carácter de interrogación (\"?\")**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos cargar el conjunto de datos usando la función `read_csv()` y especificar los `na_values` para cargar los valores de `'?'` como datos faltantes, marcados con un valor `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"https://raw.githubusercontent.com/lihkir/Data/main/horse-colic.csv\", header=None, na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Luego podemos enumerar cada columna y reportar el *número de filas con valores faltantes para esa columna*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dataframe.shape[1]):\n",
    " n_miss = dataframe[[i]].isnull().sum()\n",
    " perc = n_miss / dataframe.shape[0] * 100\n",
    " print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos observar que algunas columnas (por ejemplo, los índices de *columna 1 y 2) no tienen valores faltantes*, mientras que otras columnas (por ejemplo, los índices de *columna 15 y 21) tienen muchos o incluso la mayoría de los valores faltantes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Transformación de Datos con IterativeImputer**: Es una transformación de datos configurada según el método utilizado para estimar los valores faltantes. Por defecto, se emplea un modelo `BayesianRidge()` que utiliza una función de todas las demás características de entrada. **Las características se completan en orden ascendente, desde aquellas con menos valores faltantes hasta aquellas con más**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(n_nearest_features=None, imputation_order='ascending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El imputador se ajusta a un conjunto de datos. Luego, **el imputador ajustado se aplica a un conjunto de datos para crear una copia del conjunto de datos con todos los valores faltantes de cada columna reemplazados por un valor estimado**. La clase `IterativeImputer` no se puede utilizar directamente porque es experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer()\n",
    "imputer.fit(X)\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "print('Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IterativeImputer y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para aplicar correctamente la imputación iterativa de datos faltantes y **evitar la fuga de datos**, se crea un `pipeline` de modelado donde el **primer paso sea la imputación iterativa**, y el **segundo paso sea el modelo**. Esto se puede lograr utilizando la clase `Pipeline`. Por ejemplo, el `Pipeline` a continuación utiliza un `IterativeImputer` con la estrategia predeterminada, seguido de un **modelo de bosque aleatorio**.\n",
    "\n",
    "- Podemos evaluar el conjunto de datos imputado y el `pipeline` de modelado de bosque aleatorio para el conjunto de datos con *validación cruzada* repetida de 10 pliegues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "imputer = IterativeImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/lihkir/Data/main/horse-colic.csv'\n",
    "dataframe = read_csv(url, header=None, na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "imputer = IterativeImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Los resultados pueden variar debido a la naturaleza estocástica del algoritmo o del procedimiento de evaluación, o diferencias en la precisión numérica**. Considera ejecutar el ejemplo varias veces y comparar el resultado promedio.\n",
    "\n",
    "- El pipeline se evalúa utilizando tres repeticiones de validación cruzada de 10 pliegues y reporta una precisión media de clasificación en el conjunto de datos de aproximadamente 86.3 por ciento, lo cual es un buen puntaje. **¿Cómo sabemos que usar una estrategia iterativa predeterminada es buena o la mejor para este conjunto de datos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IterativeImputer y diferente orden de imputación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por defecto, la imputación se realiza en *orden ascendente, desde la característica con menos valores faltantes hasta la característica con más valores faltantes*. Sin embargo, podemos experimentar con diferentes estrategias de orden de imputación, como **descendente, de derecha a izquierda (árabe), de izquierda a derecha (romano) y aleatorio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "strategies = ['ascending', 'descending', 'roman', 'arabic', 'random']\n",
    "for s in strategies:\n",
    " pipeline = Pipeline(steps=[('i', IterativeImputer(imputation_order=s)), ('m', RandomForestClassifier())])\n",
    " cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    " scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    " results.append(scores)\n",
    " print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot(results, labels=strategies, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los resultados sugieren *pocas diferencias entre la mayoría de los métodos*, con el orden descendente (opuesto al predeterminado) obteniendo el mejor rendimiento. Los resultados sugieren que el **orden árabe (de derecha a izquierda) o romano podría ser mejor para este conjunto de datos** con una precisión de aproximadamente 87.2 por ciento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IterativeImputer y diferente número de iteraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por defecto, `IterativeImputer` repetirá el número de iteraciones 10 veces. Es posible que un **gran número de iteraciones comience a sesgar o distorsionar la estimación y que se prefieran pocas iteraciones**. El número de iteraciones del procedimiento se puede especificar mediante el argumento `\"max_iter\"`. El siguiente ejemplo compara diferentes valores para `\"max_iter\"` desde 1 hasta 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "strategies = [str(i) for i in range(1, 21)]\n",
    "for s in strategies:\n",
    "\tpipeline = Pipeline(steps=[('i', IterativeImputer(max_iter=int(s))), ('m', RandomForestClassifier())])\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\tresults.append(scores)\n",
    "\tprint('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot(results, labels=strategies, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los resultados sugieren que **muy pocas iteraciones, como 3, podrían ser igual de efectivas o más que 9-12 iteraciones** en este conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IterativeImputer al hacer una predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos desear crear un `pipeline` de modelado final *con la imputación iterativa y el algoritmo de bosque aleatorio*, y luego hacer una predicción para nuevos datos. Esto se puede lograr definiendo el `pipeline` y ajustándolo en todos los datos disponibles, luego llamando a la función `predict()`, pasando los nuevos datos como argumento. Es importante destacar que la fila de nuevos datos debe marcar cualquier valor faltante utilizando el valor `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('i', IterativeImputer()), ('m', RandomForestClassifier())])\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = pipeline.predict([row])\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancias y/o funciones duplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El tercer problema es la presencia de `instancias y/o características duplicadas en un conjunto de datos`. Se trata de elementos innecesarios en el conjunto de datos y, si no se eliminan, *pueden afectar a las tendencias y a los conocimientos que se muestran en una visualización*.\n",
    "\n",
    "- Por ejemplo, puede crear una visualización que muestre la relación entre el género de un adolescente y si toca el piano. Con un conjunto de datos sin valores atípicos, anomalías o valores perdidos, obtendrá una gran visualización.\n",
    "\n",
    "- A partir de la visualización, podrá también concluir que **hay más mujeres que tocan el piano que hombres**. Sin embargo, digamos que la visualización proviene del conjunto de datos siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./figures/Figure714.png\n",
    ":align: center\n",
    ":scale: 80\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hay dos instancias para **Nita Thadaka** y tres instancias para **Pooja Rajesh**, lo que significa que hay `tres instancias duplicadas en total`. Esto significa que la información que la visualización está proporcionando es inexacta. La forma de tratar los duplicados es sencilla: **eliminarlos**. Con este fin puede utlizar la función: `.drop_duplicates()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mala selección de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con respecto a un conjunto de datos, una característica es una columna en el conjunto de datos, mientras que una instancia es una fila en el conjunto de datos. Por ejemplo, en la tabla anterior, nombre, sexo, tocar el piano y la edad son características, mientras que Pooja Rajesh, F, Yes y 17 es una instancia.\n",
    "\n",
    "- Dado que el objetivo de una visualización es mostrar una tendencia, un patrón, una relación o algún vínculo entre dos o más características en un conjunto de datos, `es importante que la selección de esas características se haga con cuidado`. Por lo tanto, este es un punto crucial en la visualización de datos.\n",
    "\n",
    "- Si el objetivo es transmitir que existe una fuerte relación entre dos características, entonces hay que `asegurarse de que están fuertemente correlacionadas antes de proceder a su visualización`. La selección de características insignificantes dará lugar a una visualización sin sentido y no transmitirá ninguna información concreta. Por ejemplo, en cuanto al conjunto de datos [co2.csv](https://raw.githubusercontent.com/lihkir/Uninorte/main/AppliedStatisticMS/DataVisualizationRPython/Lectures/Python/PythonDataSets/co2.csv), el conjunto de datos contiene información sobre las emisiones de dióxido de carbono por país y el PIB por país. Comprobamos la `correlación entre las emisiones de CO2 y el PIB antes de visualizar el conjunto de datos, garantizando que íbamos a crear una\n",
    "una visualización que valiera la pena`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio para entregar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine las características que se visualizan en un gráfico de dispersión. Se le da el conjunto de datos [co2.csv](https://raw.githubusercontent.com/lihkir/Uninorte/main/AppliedStatisticMS/DataVisualizationRPython/Lectures/Python/PythonDataSets/co2.csv) y se le pide que proporcione información sobre él, como por ejemplo, qué tipo de patrones existen, si hay tendencias entre las características, etc. Es necesario que su visualización final transmita información significativa. Para conseguirlo, va a crear visualizaciones para diferentes emparejamientos de características para entender están correlacionadas y, por lo tanto, vale la pena visualizarlas.\n",
    "\n",
    "    **Pasos principales**\n",
    "    \n",
    "    - Importe las bibliotecas necesarias.\n",
    "    - Vuelva a crear el `DataFrame`. Desde el `DataFrame` `gm` incluya las columnas `population`, `fertility`, y `life`\n",
    "    - Visualiza la relación entre el `co2` y `life` utilizando un gráfico de dispersión, con el nombre del país como información en la herramienta hover y el año como deslizador.\n",
    "    - Comprueba la correlación entre `co2` y `life`.\n",
    "    - Visualiza la relación entre `co2` y `fertility` mediante un gráfico de dispersión, con el nombre del país como información en la herramienta hover y el año como deslizador.\n",
    "    - Comprueba la correlación entre `co2` y `fertility`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Realicé un EDA para el siguiente conjunto de datos (ver [Diabetes Dataset](https://raw.githubusercontent.com/lihkir/Data/main/diabetes.csv)). El conjunto de datos consta de varias variables médicas predictoras (independientes) y una variable objetivo (*diagnóstico*, dependiente), resultado. Las variables independientes incluyen el número de embarazos que ha tenido la paciente, BMI (`Body mass index` (peso en `kg/(height in m)^2`), nivel de insulina, edad, etc. En este dataset, sustituimos todos los valores nulos \"faltantes\" por NAN y realizamos el respectivo análisis de datos faltantes.\n",
    "   \n",
    "```python\n",
    "df.loc[df[\"Glucose\"] == 0.0, \"Glucose\"] = np.NAN\n",
    "df.loc[df[\"BloodPressure\"] == 0.0, \"BloodPressure\"] = np.NAN\n",
    "df.loc[df[\"SkinThickness\"] == 0.0, \"SkinThickness\"] = np.NAN\n",
    "df.loc[df[\"Insulin\"] == 0.0, \"Insulin\"] = np.NAN\n",
    "df.loc[df[\"BMI\"] == 0.0, \"BMI\"] = np.NAN\n",
    "```\n",
    "- Aplique imputación de datos usando las diferentes opciones de estimadores (ver [estimators-that-handle-nan-values](https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values)). Identifique si el problema es de clasificación o regresión previamente.\n",
    "  \n",
    "- Identifique datos atípicos para cada variable en el dataset usando las técnicas estudiadas en clase. Además, realice imputación de los datos atípicos con base en lo desarrollado en el ítem anterior (ver [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)).\n",
    "  - Imputación/Capping/Predicción\n",
    "  - Use los siguientes tests para detectar valores atípicos: Test de Rosner, Dixon, Grubbs, Hampel, Percentiles, Boxplots, Histograms, Descriptivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_tf",
   "language": "python",
   "name": "ml_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de contenido",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
